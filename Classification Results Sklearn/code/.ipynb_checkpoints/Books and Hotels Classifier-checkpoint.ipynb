{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04_02 Preparing Data for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"appen_translated - books.csv\")\n",
    "\n",
    "h = data[data[\"helpfulness\"] == \"helpful\"][:662]\n",
    "n = data[data[\"helpfulness\"] != \"helpful\"]\n",
    "\n",
    "\n",
    "data = pd.concat([h,n])\n",
    "data.head()\n",
    "\n",
    "reveiws = data['review_en']\n",
    "classes = data['helpfulness']\n",
    "classes = classes.map(lambda x: replace(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Omar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/Omar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample feature names identified :  ['-_-', '-aa', '-algrayb', '-alhabkh', '-almuslimn', '-alokther-imposed', '-aloutn', '-alvkrh', '-anfsal', '-atri', '-d', '-epeshkl', '-hepta-', '-holt', '-how', '-i', '-is', '-kadd', '-lmassalha-', '-noaa', '-oaakhz', '-oaraf', '-obataba', '-ohzh', '-oma']\n",
      "\n",
      "Size of TFIDF matrix :  (1324, 10190)\n"
     ]
    }
   ],
   "source": [
    "tfidf_books = extract_tfidf(reveiws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct = struct_extract(reveiws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found :  ['helpful' 'not_helpful']\n",
      "\n",
      "Classes converted to integers : [0 0 0 ... 1 1 1]\n",
      "NB\n",
      "Confusion Matrix : \n",
      "[[125  35]\n",
      " [122  49]]\n",
      "\n",
      " Prediction Accuracy :  0.525679758308157\n",
      "------------------------\n",
      "SVM\n",
      "Confusion Matrix : \n",
      "[[ 76  84]\n",
      " [ 26 145]]\n",
      "\n",
      " Prediction Accuracy :  0.6676737160120846\n",
      "------------------------\n",
      "Decision Tree\n",
      "Confusion Matrix : \n",
      "[[101  59]\n",
      " [ 68 103]]\n",
      "\n",
      " Prediction Accuracy :  0.6163141993957704\n",
      "------------------------\n",
      "Random Forest\n",
      "Confusion Matrix : \n",
      "[[103  57]\n",
      " [ 37 134]]\n",
      "\n",
      " Prediction Accuracy :  0.716012084592145\n",
      "------------------------\n",
      "NN\n",
      "Confusion Matrix : \n",
      "[[116  44]\n",
      " [ 50 121]]\n",
      "\n",
      " Prediction Accuracy :  0.716012084592145\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "features = struct\n",
    "extract_classify_test(features,classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hotels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>helpfulness</th>\n",
       "      <th>confidence</th>\n",
       "      <th>no</th>\n",
       "      <th>review</th>\n",
       "      <th>review_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2892106090</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/10/2020 22:54:18</td>\n",
       "      <td>helpful</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>23768</td>\n",
       "      <td>جيد. مديرة خدمة العملاء الانسه بشرى جدا متميزة...</td>\n",
       "      <td>good. Director of Customer Service Miss Bushra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2892106091</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/10/2020 22:54:18</td>\n",
       "      <td>helpful</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>173933</td>\n",
       "      <td>مقبول. موقع ممتاز. انتظار الاصنصير طويل لم يكن...</td>\n",
       "      <td>Acceptable. Excellent location. Waiting for a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2892106092</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/10/2020 22:54:18</td>\n",
       "      <td>helpful</td>\n",
       "      <td>0.6765</td>\n",
       "      <td>213748</td>\n",
       "      <td>جيد. الموقع ، طاقم العمل بشوشين ، مكان الافطار...</td>\n",
       "      <td>good. Location, staff Bchocan, place a small b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2892106093</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/10/2020 22:54:18</td>\n",
       "      <td>helpful</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>147649</td>\n",
       "      <td>“ممتازة”. كل شي رائع و نظيف و الاسرة نظيفة ومر...</td>\n",
       "      <td>\"Excellent\". Everything wonderful and clean an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2892106094</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/10/2020 22:55:10</td>\n",
       "      <td>helpful</td>\n",
       "      <td>0.6604</td>\n",
       "      <td>89990</td>\n",
       "      <td>“اروع فندق”. كل شي كان فندق حقيقة اكثر من رائع...</td>\n",
       "      <td>\"Finest hotel.\" Everything was more than a hot...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     _unit_id  _golden _unit_state  _trusted_judgments    _last_judgment_at  \\\n",
       "0  2892106090    False   finalized                   3  12/10/2020 22:54:18   \n",
       "1  2892106091    False   finalized                   3  12/10/2020 22:54:18   \n",
       "2  2892106092    False   finalized                   3  12/10/2020 22:54:18   \n",
       "3  2892106093    False   finalized                   3  12/10/2020 22:54:18   \n",
       "4  2892106094    False   finalized                   3  12/10/2020 22:55:10   \n",
       "\n",
       "  helpfulness  confidence      no  \\\n",
       "0     helpful      1.0000   23768   \n",
       "1     helpful      1.0000  173933   \n",
       "2     helpful      0.6765  213748   \n",
       "3     helpful      1.0000  147649   \n",
       "4     helpful      0.6604   89990   \n",
       "\n",
       "                                              review  \\\n",
       "0  جيد. مديرة خدمة العملاء الانسه بشرى جدا متميزة...   \n",
       "1  مقبول. موقع ممتاز. انتظار الاصنصير طويل لم يكن...   \n",
       "2  جيد. الموقع ، طاقم العمل بشوشين ، مكان الافطار...   \n",
       "3  “ممتازة”. كل شي رائع و نظيف و الاسرة نظيفة ومر...   \n",
       "4  “اروع فندق”. كل شي كان فندق حقيقة اكثر من رائع...   \n",
       "\n",
       "                                           review_en  \n",
       "0  good. Director of Customer Service Miss Bushra...  \n",
       "1  Acceptable. Excellent location. Waiting for a ...  \n",
       "2  good. Location, staff Bchocan, place a small b...  \n",
       "3  \"Excellent\". Everything wonderful and clean an...  \n",
       "4  \"Finest hotel.\" Everything was more than a hot...  "
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotels = pd.read_csv(\"appen_translated - hotels.csv\")\n",
    "\n",
    "h = hotels[hotels[\"helpfulness\"] == \"helpful\"][:641]\n",
    "n = hotels[hotels[\"helpfulness\"] != \"helpful\"]\n",
    "\n",
    "\n",
    "hotels = pd.concat([h,n])\n",
    "hotels_reviews = hotels['review_en']\n",
    "\n",
    "hotels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels_classes = hotels[\"helpfulness\"]\n",
    "hotels_classes = hotels_classes.map(lambda x: replace(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Omar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/Omar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample feature names identified :  ['-aamadat', '-almsobh-', '-and', '-astkabbal', '-okrmkm', '-raihh', '-saara', '-sar', '-shehadlah', '-srah', '-tsgel', '..ajbna', '..alavdil', '..anh', '..la', '..lona', '..nziv', '..vdltinay', '.2', '.2.', '.5-bathroom', '._', '.aaftar', '.abahama', '.adm']\n",
      "\n",
      "Size of TFIDF matrix :  (1282, 3613)\n"
     ]
    }
   ],
   "source": [
    "tfidf_hotels = extract_tfidf(hotels_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_hotels = pd.DataFrame(tfidf_hotels.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_hotels = struct_extract(hotels_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'helping/inquirerbasic.xls'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-358-22f5da476f44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minq_hotels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minq_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhotels_reviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-357-e16da26fc273>\u001b[0m in \u001b[0;36minq_extract\u001b[0;34m(reviews)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;31m#read inq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0minq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'helping/inquirerbasic.xls'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0minq_categs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, io, engine)\u001b[0m\n\u001b[1;32m    819\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstringify_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Install xlrd >= 1.0.0 for Excel support\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"xlrd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36mload_workbook\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# a ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'helping/inquirerbasic.xls'"
     ]
    }
   ],
   "source": [
    "inq_hotels = inq_extract(hotels_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_hotels = liwc_extract(hotels_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "galc_hotesl = galc_extract(hotels_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found :  ['helpful' 'not_helpful']\n",
      "\n",
      "Classes converted to integers : [0 0 0 ... 1 1 1]\n",
      "NB\n",
      "Confusion Matrix : \n",
      "[[ 20 144]\n",
      " [  6 151]]\n",
      "\n",
      " Prediction Accuracy :  0.5327102803738317\n",
      "------------------------\n",
      "SVM\n",
      "Confusion Matrix : \n",
      "[[ 75  89]\n",
      " [ 24 133]]\n",
      "\n",
      " Prediction Accuracy :  0.6479750778816199\n",
      "------------------------\n",
      "Decision Tree\n",
      "Confusion Matrix : \n",
      "[[96 68]\n",
      " [85 72]]\n",
      "\n",
      " Prediction Accuracy :  0.5233644859813084\n",
      "------------------------\n",
      "Random Forest\n",
      "Confusion Matrix : \n",
      "[[104  60]\n",
      " [ 41 116]]\n",
      "\n",
      " Prediction Accuracy :  0.6853582554517134\n",
      "------------------------\n",
      "NN\n",
      "Confusion Matrix : \n",
      "[[108  56]\n",
      " [ 51 106]]\n",
      "\n",
      " Prediction Accuracy :  0.6666666666666666\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:470: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "####### Choose features \n",
    "features = struct\n",
    "\n",
    "####### run \n",
    "extract_classify_test(features,hotels_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helping methods (run first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_classify_test(features,classes):\n",
    "    \n",
    "    ####### Build the model \n",
    "    \n",
    "    #Create Labels and integer classes\n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(classes)\n",
    "    print(\"Classes found : \", le.classes_)\n",
    "\n",
    "    #Convert classes to integers for use with ML\n",
    "    int_classes = le.transform(classes)\n",
    "    print(\"\\nClasses converted to integers :\", int_classes)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    #Split as training and testing sets\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(features, int_classes,random_state=0)\n",
    "    \n",
    "    \n",
    "    ####### Classify & Test\n",
    "    \n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    classifier_1 = MultinomialNB().fit(xtrain, ytrain)\n",
    "    print(\"NB\")\n",
    "    test(classifier_1,xtest,ytest)\n",
    "\n",
    "    from sklearn import svm\n",
    "    classifier_2 = svm.SVC(kernel='linear').fit(xtrain, ytrain)\n",
    "    print(\"SVM\")\n",
    "    test(classifier_2,xtest,ytest)\n",
    "\n",
    "    from sklearn import tree\n",
    "    clf_3 = tree.DecisionTreeClassifier().fit(xtrain, ytrain)\n",
    "    print(\"Decision Tree\")\n",
    "    test(clf_3,xtest,ytest)\n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    clf_4 = RandomForestClassifier(max_depth=2, random_state=0).fit(xtrain,ytrain)\n",
    "    print(\"Random Forest\")\n",
    "    test(clf_4,xtest,ytest)\n",
    "\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    clf_5 = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(20, 2), random_state=0).fit(xtrain,ytrain)\n",
    "    print(\"NN\")\n",
    "    test(clf_5,xtest,ytest)\n",
    "    \n",
    "def replace(x):\n",
    "    if x != \"helpful\":\n",
    "        return \"not_helpful\"\n",
    "    else:\n",
    "        return \"helpful\"\n",
    "\n",
    "def test(clf,xtest,ytest):\n",
    "    from sklearn import metrics\n",
    "    #Predict on test data\n",
    "    predictions=clf.predict(xtest)\n",
    "    print(\"Confusion Matrix : \")\n",
    "    print(metrics.confusion_matrix(ytest, predictions))\n",
    "    print(\"\\n Prediction Accuracy : \",  \\\n",
    "          metrics.accuracy_score(ytest, predictions) )\n",
    "    print(\"------------------------\")\n",
    "    \n",
    "####### Features methods \n",
    "\n",
    "\n",
    "def extract_tfidf(reviews):\n",
    "        \n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    #setup wordnet for lemmatization\n",
    "    nltk.download('wordnet')\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    #Custom tokenizer that will perform tokenization, stopword removal\n",
    "    #and lemmatization\n",
    "    def customtokenize(str):\n",
    "        tokens=nltk.word_tokenize(str)\n",
    "\n",
    "        #Replace special characters\n",
    "        token_list2 = [word.replace(\"'\", \"\") for word in tokens ]\n",
    "\n",
    "        #Remove punctuations\n",
    "        token_list3 = list(filter(lambda token: nltk.tokenize.punkt.PunktToken(token).is_non_punct, token_list2))\n",
    "\n",
    "        #Convert to lower case\n",
    "        token_list4=[word.lower() for word in token_list3 ]\n",
    "\n",
    "        #remove stop words\n",
    "        nostop = list(filter(lambda token: token not in stopwords.words('english'), token_list4))\n",
    "\n",
    "        #lemmatized\n",
    "        lemmatized=[lemmatizer.lemmatize(word) for word in nostop ]\n",
    "\n",
    "        return lemmatized\n",
    "\n",
    "    #Generate TFIDF matrix\n",
    "    vectorizer = TfidfVectorizer(tokenizer=customtokenize)\n",
    "    tfidf = vectorizer.fit_transform(reviews)\n",
    "\n",
    "    print(\"\\nSample feature names identified : \", vectorizer.get_feature_names()[:25])\n",
    "    print(\"\\nSize of TFIDF matrix : \",tfidf.shape)\n",
    "\n",
    "    return tfidf\n",
    "    \n",
    "def struct_extract(reviews):\n",
    "\n",
    "    \"\"\"\n",
    "    review list(str): list of sentences \n",
    "    \"\"\"\n",
    "    \n",
    "    #initiate dataframe\n",
    "    results = pd.DataFrame(reviews)\n",
    "    results.columns = ['review']\n",
    "\n",
    "    #define local funcs\n",
    "    def avg_sent_length(string):\n",
    "        sentences = string.split('.')\n",
    "        sum_len = 0        \n",
    "        for i,s in enumerate(sentences):\n",
    "            sum_len += len(s)\n",
    "        return sum_len/len(sentences)\n",
    "\n",
    "    def per_of_q(string):\n",
    "    \n",
    "        num_q = string.count(\"?\")\n",
    "        new_string = string.replace(\"?\",\".\")\n",
    "        sentences = new_string.split(\".\")\n",
    "        \n",
    "        return num_q/len(sentences)\n",
    "    \n",
    "    #extract feats\n",
    "    results['length'] = results.review.apply(lambda x: len(x))\n",
    "    results['num_tokens'] = results.review.apply(lambda x: len(x.split(' ')))\n",
    "    results['num_sentences'] = results.review.apply(lambda x: x.count('.'))\n",
    "    results['avg_sent_len'] = results.review.apply(lambda x: avg_sent_length(x))\n",
    "    results['num_exclm_mark'] = results.review.apply(lambda x: x.count('!'))\n",
    "    results['ratio_q'] = results.review.apply(lambda x: per_of_q(x))\n",
    "    \n",
    "    #drop the review\n",
    "    results = results.drop(columns=['review'])\n",
    "    \n",
    "    #scale\n",
    "    results = results - results.min()\n",
    "    results = results / results.max()\n",
    "    results = results.fillna(0)\n",
    "    \n",
    "    #save file\n",
    "    return results\n",
    "\n",
    "# GALC\n",
    "\n",
    "def galc_extract(reviews):\n",
    "    \n",
    "    # read galc dictionary\n",
    "    with open('galc_dict.json') as json_file:\n",
    "        galc_dict = json.load(json_file)\n",
    "    \n",
    "    #init dataframe\n",
    "    galc_feature = pd.DataFrame(np.zeros((len(reviews),len(galc_dict))))\n",
    "    galc_feature.columns = list(galc_dict.keys())\n",
    "\n",
    "    def galc_vector_feature(review):\n",
    "        ps = PorterStemmer()\n",
    "        dic = dict.fromkeys(galc_dict.keys(),0)\n",
    "\n",
    "        for w in review.split(' '):\n",
    "            word = w.replace('.','')\n",
    "            stemmed = ps.stem(word)\n",
    "\n",
    "            for categ,words in galc_dict.items():\n",
    "                if stemmed in words:\n",
    "                    dic[categ] += 1\n",
    "\n",
    "        return dic.values()\n",
    "\n",
    "    for i,r in galc_feature.iterrows():\n",
    "        galc_feature.iloc[i] = galc_vector_feature(reviews[i])\n",
    "        \n",
    "    \n",
    "    #scale\n",
    "    galc_feature = galc_feature - galc_feature.min()\n",
    "    galc_feature = galc_feature / galc_feature.max()\n",
    "    galc_feature = galc_feature.fillna(0)\n",
    "\n",
    "    #Save file\n",
    "    return galc_feature\n",
    "\n",
    "\n",
    "# LIWC \n",
    "\n",
    "def liwc_extract(reviews):\n",
    "    parse, category_names = liwc.load_token_parser('LIWC2007_English100131.dic')\n",
    "\n",
    "    # define helpers\n",
    "    def tokenize(text):\n",
    "        # you may want to use a smarter tokenizer\n",
    "        for match in re.finditer(r'\\w+', text, re.UNICODE):\n",
    "            yield match.group(0)\n",
    "\n",
    "    def liwc_features(text):\n",
    "\n",
    "        dic = dict.fromkeys(category_names,0)\n",
    "\n",
    "        gettysburg_tokens = tokenize(text)\n",
    "        gettysburg_counts = Counter(category for token in gettysburg_tokens for category in parse(token))\n",
    "\n",
    "        for k,v in gettysburg_counts.items():\n",
    "            dic[k] = v\n",
    "\n",
    "        return dic.values()\n",
    "    \n",
    "    # init dataframe\n",
    "    liwc_feature = pd.DataFrame(np.zeros((len(reviews),len(category_names))))\n",
    "    liwc_feature.columns = category_names\n",
    "    \n",
    "    #extract feats\n",
    "    for i,r in liwc_feature.iterrows():\n",
    "        liwc_feature.iloc[i] = liwc_features(reviews[i])\n",
    "    \n",
    "    #scale\n",
    "    liwc_feature = liwc_feature - liwc_feature.min()\n",
    "    liwc_feature = liwc_feature / liwc_feature.max()\n",
    "    liwc_feature = liwc_feature.fillna(0)\n",
    "\n",
    "    #save file\n",
    "    return liwc_feature\n",
    "\n",
    "# INQURIER \n",
    "\n",
    "def inq_extract(reviews):\n",
    "    \n",
    "    #read inq\n",
    "    inq = pd.read_excel('inquirerbasic.xls')\n",
    "    inq_categs = list(inq.columns)\n",
    "    \n",
    "    #init dataframe\n",
    "    inq_features = np.zeros((1,len(inq_categs)),dtype=int)\n",
    "\n",
    "    #extract features \n",
    "    for review in reviews:\n",
    "        inq_feat = dict.fromkeys(inq_categs,0)\n",
    "        for w in review.split(' '):\n",
    "            clean = w.strip().replace('.',\"\").replace(\"?\",'').replace(\",\",\"\").replace(\";\",'').upper()\n",
    "            # if the word exists in the dictionary\n",
    "            if len(inq[inq['Entry'] == clean]) > 0:\n",
    "                row = inq[inq['Entry']==clean].to_dict()\n",
    "                for k,v in row.items():\n",
    "                    vv = list(v.values())[0]\n",
    "                    if isinstance(vv,str):\n",
    "                        inq_feat[k] += 1\n",
    "\n",
    "        # convert the dict to one row features \n",
    "        inq_feat_row = np.array(list(inq_feat.values()),dtype=int).reshape((1,len(inq_categs)))\n",
    "\n",
    "        #combine with big matrix\n",
    "        inq_features = np.concatenate((inq_features,inq_feat_row),axis=0)\n",
    "    \n",
    "    \n",
    "    #scale\n",
    "    inq_features = pd.DataFrame(inq_features)\n",
    "    inq_features = inq_features - inq_features.min()\n",
    "    inq_features = inq_features / inq_features.max()\n",
    "    inq_features = inq_features.fillna(0)\n",
    "    \n",
    "    # save file\n",
    "    return inq_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
