{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>helpfulness</th>\n",
       "      <th>confidence</th>\n",
       "      <th>no</th>\n",
       "      <th>review</th>\n",
       "      <th>review_en</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2905523768</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/23/2020 7:25:58</td>\n",
       "      <td>somewhat_helpful</td>\n",
       "      <td>0.6598</td>\n",
       "      <td>64963</td>\n",
       "      <td>مهما كانت ميولك الفكريه .. لازم هتحب حاجه في ا...</td>\n",
       "      <td>Whatever the intellectual tastes .. necessary ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2905523769</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/23/2020 7:25:58</td>\n",
       "      <td>helpful</td>\n",
       "      <td>0.6598</td>\n",
       "      <td>52854</td>\n",
       "      <td>.. .. هنا التقيت بأحد أعجب الرجال اطلاقا،شمس ا...</td>\n",
       "      <td>.. .. here I met one impressed by the men at a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2905523770</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/23/2020 7:25:58</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>0.6598</td>\n",
       "      <td>48057</td>\n",
       "      <td>سأعود</td>\n",
       "      <td>I'll come back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2905523771</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/23/2020 7:25:58</td>\n",
       "      <td>helpful</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>14611</td>\n",
       "      <td>لم أكن قد قرأتُ لأحلام ثلاثيتها الشهيرة، ولم أ...</td>\n",
       "      <td>I had not read the dreams of the famous Thelat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2905523772</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>12/23/2020 7:54:33</td>\n",
       "      <td>helpful</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>17368</td>\n",
       "      <td>تسحرني لغتها وقلمها. تمنيت أن صفحات الكتاب لا ...</td>\n",
       "      <td>Language fascinates me and her pen. I wished t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     _unit_id  _golden _unit_state  _trusted_judgments   _last_judgment_at  \\\n",
       "0  2905523768    False   finalized                   3  12/23/2020 7:25:58   \n",
       "1  2905523769    False   finalized                   3  12/23/2020 7:25:58   \n",
       "2  2905523770    False   finalized                   3  12/23/2020 7:25:58   \n",
       "3  2905523771    False   finalized                   3  12/23/2020 7:25:58   \n",
       "4  2905523772    False   finalized                   3  12/23/2020 7:54:33   \n",
       "\n",
       "        helpfulness  confidence     no  \\\n",
       "0  somewhat_helpful      0.6598  64963   \n",
       "1           helpful      0.6598  52854   \n",
       "2         unrelated      0.6598  48057   \n",
       "3           helpful      1.0000  14611   \n",
       "4           helpful      1.0000  17368   \n",
       "\n",
       "                                              review  \\\n",
       "0  مهما كانت ميولك الفكريه .. لازم هتحب حاجه في ا...   \n",
       "1  .. .. هنا التقيت بأحد أعجب الرجال اطلاقا،شمس ا...   \n",
       "2                                              سأعود   \n",
       "3  لم أكن قد قرأتُ لأحلام ثلاثيتها الشهيرة، ولم أ...   \n",
       "4  تسحرني لغتها وقلمها. تمنيت أن صفحات الكتاب لا ...   \n",
       "\n",
       "                                           review_en  \n",
       "0  Whatever the intellectual tastes .. necessary ...  \n",
       "1  .. .. here I met one impressed by the men at a...  \n",
       "2                                     I'll come back  \n",
       "3  I had not read the dreams of the famous Thelat...  \n",
       "4  Language fascinates me and her pen. I wished t...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_data = pd.read_csv(\"appen_translated - books.csv\")\n",
    "books_conf = books_data[(books_data[\"confidence\"] == 1.0) & (books_data[\"helpfulness\"] == \"helpful\") | (books_data[\"helpfulness\"] != \"helpful\")]\n",
    "books_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_conf = books_conf[\"helpfulness\"].map(lambda x: \"not_helpful\" if x != \"helpful\" else \"helpful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1480"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classes_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2404"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(books_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_full = struct_extract(books_data[\"review_en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>avg_sent_len</th>\n",
       "      <th>num_exclm_mark</th>\n",
       "      <th>ratio_q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005994</td>\n",
       "      <td>0.004958</td>\n",
       "      <td>0.015707</td>\n",
       "      <td>0.015413</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.085801</td>\n",
       "      <td>0.084284</td>\n",
       "      <td>0.083770</td>\n",
       "      <td>0.056058</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000984</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012539</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.122931</td>\n",
       "      <td>0.122955</td>\n",
       "      <td>0.047120</td>\n",
       "      <td>0.140857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009573</td>\n",
       "      <td>0.009420</td>\n",
       "      <td>0.010471</td>\n",
       "      <td>0.035528</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     length  num_tokens  num_sentences  avg_sent_len  num_exclm_mark  ratio_q\n",
       "0  0.005994    0.004958       0.015707      0.015413             0.0      0.0\n",
       "1  0.085801    0.084284       0.083770      0.056058             0.0      0.0\n",
       "2  0.000984    0.000992       0.000000      0.012539             0.0      0.0\n",
       "3  0.122931    0.122955       0.047120      0.140857             0.0      0.0\n",
       "4  0.009573    0.009420       0.010471      0.035528             0.0      0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_conf = struct_full.reindex(books_conf.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1480"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(struct_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found :  ['helpful' 'not_helpful']\n",
      "\n",
      "Classes converted to integers : [1 1 0 ... 0 1 1]\n",
      "NB\n",
      "Confusion Matrix : \n",
      "[[81  5]\n",
      " [59  3]]\n",
      "Acc:  0.568  Prec:  0.375  Recall:  0.048  F1: 0.086\n",
      "SVM\n",
      "Confusion Matrix : \n",
      "[[57 29]\n",
      " [ 6 56]]\n",
      "Acc:  0.764  Prec:  0.659  Recall:  0.903  F1: 0.762\n",
      "Decision Tree\n",
      "Confusion Matrix : \n",
      "[[66 20]\n",
      " [15 47]]\n",
      "Acc:  0.764  Prec:  0.701  Recall:  0.758  F1: 0.729\n",
      "Random Forest\n",
      "Confusion Matrix : \n",
      "[[69 17]\n",
      " [ 9 53]]\n",
      "Acc:  0.824  Prec:  0.757  Recall:  0.855  F1: 0.803\n",
      "NN\n",
      "Confusion Matrix : \n",
      "[[71 15]\n",
      " [11 51]]\n",
      "Acc:  0.824  Prec:  0.773  Recall:  0.823  F1: 0.797\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NB': (array([[81,  5],\n",
       "         [59,  3]]),\n",
       "  0.5675675675675675,\n",
       "  0.375,\n",
       "  0.04838709677419355,\n",
       "  0.0857142857142857),\n",
       " 'SVM': (array([[57, 29],\n",
       "         [ 6, 56]]),\n",
       "  0.7635135135135135,\n",
       "  0.6588235294117647,\n",
       "  0.9032258064516129,\n",
       "  0.761904761904762),\n",
       " 'DT': (array([[66, 20],\n",
       "         [15, 47]]),\n",
       "  0.7635135135135135,\n",
       "  0.7014925373134329,\n",
       "  0.7580645161290323,\n",
       "  0.7286821705426356),\n",
       " 'RF': (array([[69, 17],\n",
       "         [ 9, 53]]),\n",
       "  0.8243243243243243,\n",
       "  0.7571428571428571,\n",
       "  0.8548387096774194,\n",
       "  0.803030303030303),\n",
       " 'NN': (array([[71, 15],\n",
       "         [11, 51]]),\n",
       "  0.8243243243243243,\n",
       "  0.7727272727272727,\n",
       "  0.8225806451612904,\n",
       "  0.796875)}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_classify_test(struct_conf,classes_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "galc = galc_extract(books_data[\"review_en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "galc_conf = galc.reindex(books_conf.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes found :  ['helpful' 'not_helpful']\n",
      "\n",
      "Classes converted to integers : [1 1 0 ... 0 1 1]\n",
      "NB\n",
      "Confusion Matrix : \n",
      "[[81  5]\n",
      " [62  0]]\n",
      "Acc:  0.547  Prec:  0.0  Recall:  0.0  F1: 0.0\n",
      "SVM\n",
      "Confusion Matrix : \n",
      "[[39 47]\n",
      " [10 52]]\n",
      "Acc:  0.615  Prec:  0.525  Recall:  0.839  F1: 0.646\n",
      "Decision Tree\n",
      "Confusion Matrix : \n",
      "[[48 38]\n",
      " [11 51]]\n",
      "Acc:  0.669  Prec:  0.573  Recall:  0.823  F1: 0.675\n",
      "Random Forest\n",
      "Confusion Matrix : \n",
      "[[86  0]\n",
      " [62  0]]\n",
      "Acc:  0.581  Prec:  0.0  Recall:  0.0  F1: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN\n",
      "Confusion Matrix : \n",
      "[[54 32]\n",
      " [12 50]]\n",
      "Acc:  0.703  Prec:  0.61  Recall:  0.806  F1: 0.694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NB': (array([[81,  5],\n",
       "         [62,  0]]),\n",
       "  0.5472972972972973,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0),\n",
       " 'SVM': (array([[39, 47],\n",
       "         [10, 52]]),\n",
       "  0.6148648648648649,\n",
       "  0.5252525252525253,\n",
       "  0.8387096774193549,\n",
       "  0.6459627329192548),\n",
       " 'DT': (array([[48, 38],\n",
       "         [11, 51]]),\n",
       "  0.668918918918919,\n",
       "  0.5730337078651685,\n",
       "  0.8225806451612904,\n",
       "  0.6754966887417219),\n",
       " 'RF': (array([[86,  0],\n",
       "         [62,  0]]),\n",
       "  0.581081081081081,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0),\n",
       " 'NN': (array([[54, 32],\n",
       "         [12, 50]]),\n",
       "  0.7027027027027027,\n",
       "  0.6097560975609756,\n",
       "  0.8064516129032258,\n",
       "  0.6944444444444445)}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_classify_test(galc_conf,classes_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Omar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/Omar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample feature names identified :  ['-115', '-241', '-242', '-_-', '-aa', '-aasy-', '-adman', '-adtham', '-ahadjab', '-aharov', '-ahiana', '-algrayb', '-alhab', '-alhabkh', '-alkon', '-allah', '-alm', '-almuslimn', '-alnhih', '-alokther-imposed', '-aloutn', '-alshl', '-alta', '-alvkrh', '-aly']\n",
      "\n",
      "Size of TFIDF matrix :  (2404, 16508)\n"
     ]
    }
   ],
   "source": [
    "tfidf = extract_tfidf(books_data[\"review_en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2404\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0, 3732)\\t0.18331583255178605\\n  (0, 10650)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0, 6112)\\t0.09604038547213667\\n  (0, 7871)\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0, 2836)\\t0.7184002324136433\\n  (0, 4474)\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0, 12663)\\t0.07842194439589507\\n  (0, 9265)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0, 2808)\\t0.5438280087036974\\n  (0, 5945)\\t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0    (0, 3732)\\t0.18331583255178605\\n  (0, 10650)...\n",
       "1    (0, 6112)\\t0.09604038547213667\\n  (0, 7871)\\...\n",
       "2    (0, 2836)\\t0.7184002324136433\\n  (0, 4474)\\t...\n",
       "3    (0, 12663)\\t0.07842194439589507\\n  (0, 9265)...\n",
       "4    (0, 2808)\\t0.5438280087036974\\n  (0, 5945)\\t..."
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HELPING METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "import re\n",
    "from collections import Counter\n",
    "import liwc\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def extract_classify_test(features,classes,kf=0):\n",
    "    \n",
    "    f_kbest_k = features\n",
    "    \n",
    "    if kf != 0:\n",
    "        sel_k = SelectKBest(chi2, k=kf)\n",
    "        f_kbest_k = sel_k.fit_transform(features, classes)\n",
    "    \n",
    "    ####### Build the model \n",
    "    \n",
    "    #Create Labels and integer classes\n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(classes)\n",
    "    print(\"Classes found : \", le.classes_)\n",
    "\n",
    "    #Convert classes to integers for use with ML\n",
    "    int_classes = le.transform(classes)\n",
    "    print(\"\\nClasses converted to integers :\", int_classes)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    #Split as training and testing sets\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(f_kbest_k, int_classes,random_state=1,test_size=0.1)\n",
    "    \n",
    "    \n",
    "    ####### Classify & Test\n",
    "    \n",
    "    cm_acc = {\"NB\":(),\"SVM\":(),\"DT\":(),\"RF\":(),\"NN\":()}\n",
    "    \n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    classifier_1 = MultinomialNB().fit(xtrain, ytrain)\n",
    "    print(\"NB\")\n",
    "    cm_acc[\"NB\"] = test(classifier_1,xtest,ytest)\n",
    "\n",
    "    from sklearn import svm\n",
    "    classifier_2 = svm.SVC(kernel='linear').fit(xtrain, ytrain)\n",
    "    print(\"SVM\")\n",
    "    cm_acc[\"SVM\"] = test(classifier_2,xtest,ytest)\n",
    "\n",
    "    from sklearn import tree\n",
    "    clf_3 = tree.DecisionTreeClassifier().fit(xtrain, ytrain)\n",
    "    print(\"Decision Tree\")\n",
    "    cm_acc[\"DT\"] = test(clf_3,xtest,ytest)\n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    clf_4 = RandomForestClassifier(max_depth=2, random_state=0).fit(xtrain,ytrain)\n",
    "    print(\"Random Forest\")\n",
    "    cm_acc[\"RF\"] = test(clf_4,xtest,ytest)\n",
    "\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    clf_5 = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(100, 2), random_state=0, max_iter=10000).fit(xtrain,ytrain)\n",
    "    print(\"NN\")\n",
    "    cm_acc[\"NN\"] = test(clf_5,xtest,ytest)\n",
    "    \n",
    "    return cm_acc\n",
    "    \n",
    "def replace(x):\n",
    "    if x != \"helpful\":\n",
    "        return \"not_helpful\"\n",
    "    else:\n",
    "        return \"helpful\"\n",
    "\n",
    "def test(clf,xtest,ytest):\n",
    "    from sklearn import metrics\n",
    "    \n",
    "    #Predict on test data\n",
    "    predictions=clf.predict(xtest)\n",
    "    \n",
    "    print(\"Confusion Matrix : \")\n",
    "    cm = metrics.confusion_matrix(ytest, predictions)\n",
    "    print(cm)\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(ytest, predictions)\n",
    "    prc = metrics.precision_score(ytest , predictions)\n",
    "    recall = metrics.recall_score(ytest , predictions)\n",
    "    f1 = metrics.f1_score(ytest , predictions)\n",
    "    \n",
    "    dec = 3\n",
    "    \n",
    "    print(\"Acc: \",round(accuracy,dec),\" Prec: \",round(prc,dec),\" Recall: \",round(recall,dec),\" F1:\",round(f1,dec))\n",
    "    \n",
    "    #if predictions.sum() != 0:\n",
    "    #    precision = metrics.precision_score(ytest , predictions)\n",
    "    #else:dsd\n",
    "    #    precision = 0.0\n",
    "    #print(\"Precision:\",precision)\n",
    "    #\n",
    "    #print(\"------------------------\")\n",
    "    \n",
    "    \n",
    "    return(cm,accuracy,prc,recall,f1)\n",
    "    #return(cm,accuracy,precision)\n",
    "    \n",
    "####### Features methods \n",
    "\n",
    "\n",
    "def extract_tfidf(reviews):\n",
    "        \n",
    "    import nltk\n",
    "    nltk.download('stopwords')\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    #setup wordnet for lemmatization\n",
    "    nltk.download('wordnet')\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    #Custom tokenizer that will perform tokenization, stopword removal\n",
    "    #and lemmatization\n",
    "    def customtokenize(str):\n",
    "        tokens=nltk.word_tokenize(str)\n",
    "\n",
    "        #Replace special characters\n",
    "        token_list2 = [word.replace(\"'\", \"\") for word in tokens ]\n",
    "\n",
    "        #Remove punctuations\n",
    "        token_list3 = list(filter(lambda token: nltk.tokenize.punkt.PunktToken(token).is_non_punct, token_list2))\n",
    "\n",
    "        #Convert to lower case\n",
    "        token_list4=[word.lower() for word in token_list3 ]\n",
    "\n",
    "        #remove stop words\n",
    "        nostop = list(filter(lambda token: token not in stopwords.words('english'), token_list4))\n",
    "\n",
    "        #lemmatized\n",
    "        lemmatized=[lemmatizer.lemmatize(word) for word in nostop ]\n",
    "\n",
    "        return lemmatized\n",
    "\n",
    "    #Generate TFIDF matrix\n",
    "    vectorizer = TfidfVectorizer(tokenizer=customtokenize)\n",
    "    tfidf = vectorizer.fit_transform(reviews)\n",
    "\n",
    "    print(\"\\nSample feature names identified : \", vectorizer.get_feature_names()[:25])\n",
    "    print(\"\\nSize of TFIDF matrix : \",tfidf.shape)\n",
    "\n",
    "    return tfidf\n",
    "    \n",
    "def struct_extract(reviews):\n",
    "\n",
    "    \"\"\"\n",
    "    review list(str): list of sentences \n",
    "    \"\"\"\n",
    "    \n",
    "    #initiate dataframe\n",
    "    results = pd.DataFrame(reviews)\n",
    "    results.columns = ['review']\n",
    "\n",
    "    #define local funcs\n",
    "    def avg_sent_length(string):\n",
    "        sentences = string.split('.')\n",
    "        sum_len = 0        \n",
    "        for i,s in enumerate(sentences):\n",
    "            sum_len += len(s)\n",
    "        return sum_len/len(sentences)\n",
    "\n",
    "    def per_of_q(string):\n",
    "    \n",
    "        num_q = string.count(\"?\")\n",
    "        new_string = string.replace(\"?\",\".\")\n",
    "        sentences = new_string.split(\".\")\n",
    "        \n",
    "        return num_q/len(sentences)\n",
    "    \n",
    "    #extract feats\n",
    "    results['length'] = results.review.apply(lambda x: len(x))\n",
    "    results['num_tokens'] = results.review.apply(lambda x: len(x.split(' ')))\n",
    "    results['num_sentences'] = results.review.apply(lambda x: x.count('.'))\n",
    "    results['avg_sent_len'] = results.review.apply(lambda x: avg_sent_length(x))\n",
    "    results['num_exclm_mark'] = results.review.apply(lambda x: x.count('!'))\n",
    "    results['ratio_q'] = results.review.apply(lambda x: per_of_q(x))\n",
    "    \n",
    "    #drop the review\n",
    "    results = results.drop(columns=['review'])\n",
    "    \n",
    "    #scale\n",
    "    results = results - results.min()\n",
    "    results = results / results.max()\n",
    "    results = results.fillna(0)\n",
    "    \n",
    "    #save file\n",
    "    return results\n",
    "\n",
    "# GALC\n",
    "\n",
    "def galc_extract(reviews):\n",
    "    \n",
    "    # read galc dictionary\n",
    "    with open('galc_dict.json') as json_file:\n",
    "        galc_dict = json.load(json_file)\n",
    "    \n",
    "    #init dataframe\n",
    "    galc_feature = pd.DataFrame(np.zeros((len(reviews),len(galc_dict))))\n",
    "    galc_feature.columns = list(galc_dict.keys())\n",
    "\n",
    "    def galc_vector_feature(review):\n",
    "        ps = PorterStemmer()\n",
    "        dic = dict.fromkeys(galc_dict.keys(),0)\n",
    "\n",
    "        for w in review.split(' '):\n",
    "            word = w.replace('.','')\n",
    "            stemmed = ps.stem(word)\n",
    "\n",
    "            for categ,words in galc_dict.items():\n",
    "                if stemmed in words:\n",
    "                    dic[categ] += 1\n",
    "\n",
    "        return dic.values()\n",
    "\n",
    "    for i,r in galc_feature.iterrows():\n",
    "        galc_feature.iloc[i] = galc_vector_feature(reviews[i])\n",
    "        \n",
    "    \n",
    "    #scale\n",
    "    galc_feature = galc_feature - galc_feature.min()\n",
    "    galc_feature = galc_feature / galc_feature.max()\n",
    "    galc_feature = galc_feature.fillna(0)\n",
    "\n",
    "    #Save file\n",
    "    return galc_feature\n",
    "\n",
    "\n",
    "# LIWC \n",
    "\n",
    "def liwc_extract(reviews):\n",
    "    import liwc\n",
    "    parse, category_names = liwc.load_token_parser('LIWC2007_English100131.dic')\n",
    "\n",
    "    # define helpers\n",
    "    def tokenize(text):\n",
    "        # you may want to use a smarter tokenizer\n",
    "        for match in re.finditer(r'\\w+', text, re.UNICODE):\n",
    "            yield match.group(0)\n",
    "\n",
    "    def liwc_features(text):\n",
    "        \n",
    "        dic = dict.fromkeys(category_names,0)\n",
    "\n",
    "        gettysburg_tokens = tokenize(text)\n",
    "        from collections import Counter\n",
    "        gettysburg_counts = Counter(category for token in gettysburg_tokens for category in parse(token))\n",
    "\n",
    "        for k,v in gettysburg_counts.items():\n",
    "            dic[k] = v\n",
    "\n",
    "        return dic.values()\n",
    "    \n",
    "    # init dataframe\n",
    "    liwc_feature = pd.DataFrame(np.zeros((len(reviews),len(category_names))))\n",
    "    liwc_feature.columns = category_names\n",
    "    \n",
    "    #extract feats\n",
    "    for i,r in liwc_feature.iterrows():\n",
    "        liwc_feature.iloc[i] = liwc_features(reviews[i])\n",
    "    \n",
    "    #scale\n",
    "    liwc_feature = liwc_feature - liwc_feature.min()\n",
    "    liwc_feature = liwc_feature / liwc_feature.max()\n",
    "    liwc_feature = liwc_feature.fillna(0)\n",
    "\n",
    "    #save file\n",
    "    return liwc_feature\n",
    "\n",
    "# INQURIER \n",
    "\n",
    "def inq_extract(reviews):\n",
    "    \n",
    "    #read inq\n",
    "    inq = pd.read_excel('inquirerbasic.xls')\n",
    "    inq_categs = list(inq.columns)\n",
    "    \n",
    "    #init dataframe\n",
    "    inq_features = np.zeros((1,len(inq_categs)),dtype=int)\n",
    "\n",
    "    #extract features \n",
    "    for review in reviews:\n",
    "        inq_feat = dict.fromkeys(inq_categs,0)\n",
    "        for w in review.split(' '):\n",
    "            clean = w.strip().replace('.',\"\").replace(\"?\",'').replace(\",\",\"\").replace(\";\",'').upper()\n",
    "            # if the word exists in the dictionary\n",
    "            if len(inq[inq['Entry'] == clean]) > 0:\n",
    "                row = inq[inq['Entry']==clean].to_dict()\n",
    "                for k,v in row.items():\n",
    "                    vv = list(v.values())[0]\n",
    "                    if isinstance(vv,str):\n",
    "                        inq_feat[k] += 1\n",
    "\n",
    "        # convert the dict to one row features \n",
    "        inq_feat_row = np.array(list(inq_feat.values()),dtype=int).reshape((1,len(inq_categs)))\n",
    "\n",
    "        #combine with big matrix\n",
    "        inq_features = np.concatenate((inq_features,inq_feat_row),axis=0)\n",
    "    \n",
    "    \n",
    "    #scale\n",
    "    inq_features = pd.DataFrame(inq_features)\n",
    "    inq_features = inq_features - inq_features.min()\n",
    "    inq_features = inq_features / inq_features.max()\n",
    "    inq_features = inq_features.fillna(0)\n",
    "    \n",
    "    # save file\n",
    "    return inq_features\n",
    "\n",
    "# extract aspects\n",
    "\n",
    "def extract_aspects(reviews,aspects):\n",
    "    aspect_reviews = np.zeros((len(reviews),len(aspects)))\n",
    "    for i,review in enumerate(reviews):\n",
    "        for j,aspect in enumerate(aspects):\n",
    "            #count the number of occurances         \n",
    "            aspect_reviews[i][j] = review.count(aspect)\n",
    "    return aspect_reviews\n",
    "\n",
    "def extract_aspects_df(reviews,aspects):\n",
    "    \n",
    "    aspect_reviews = np.zeros((len(reviews),len(aspects)))\n",
    "    \n",
    "    for i,review in enumerate(reviews):\n",
    "        for j,aspect in enumerate(aspects):\n",
    "            #count the number of occurances    \n",
    "            print(aspect)\n",
    "            aspect_reviews[i][j] = review.count(aspect)\n",
    "    \n",
    "    #make df\n",
    "    aspect_reviews = pd.DataFrame(aspect_reviews)\n",
    "    aspect_reviews.columns = aspects\n",
    "    \n",
    "    return aspect_reviews\n",
    "\n",
    "# find max accuracy  \n",
    "\n",
    "def find_max_acc(cm_acc):\n",
    "    accs = [a[1] for a in cm_acc.values()]\n",
    "    return max(accs)\n",
    "\n",
    "def find_max_prec(cm_acc_pr):\n",
    "    prcs = [a[2] for a in cm_acc_pr.values()]\n",
    "    return max(prcs)\n",
    "    ml\n",
    "# join features \n",
    "\n",
    "def join_features(features_list):\n",
    "    features = pd.DataFrame(features_list[0])\n",
    "    for i in range(1,len(features_list)):\n",
    "        f_2 = pd.DataFrame(features_list[i])\n",
    "        cols_to_use = f_2.columns.difference(features.columns)\n",
    "        features = features.join(f_2[cols_to_use])\n",
    "    return features\n",
    "\n",
    "def join_features_df(dfs_features_list):\n",
    "    features = dfs_features_list[0]\n",
    "    for i in range(1,len(dfs_features_list)):\n",
    "        f_2 = dfs_features_list[i]\n",
    "        cols_to_use = f_2.columns.difference(features.columns)\n",
    "        features = features.join(f_2[cols_to_use])\n",
    "    return features\n",
    "    \n",
    "# combine not helpful \n",
    "\n",
    "def combine_not_helpful(classes):\n",
    "    return classes.map(lambda x: \"not_helpful\" if x != \"helpful\" else \"helpful\")\n",
    "\n",
    "\n",
    "# extract test with best k\n",
    "\n",
    "def ex_with_best_k(f,c,ks):\n",
    "    \n",
    "    # find all ks\n",
    "    \n",
    "    cm_accs = []\n",
    "    \n",
    "    for i in range(1,ks):\n",
    "        print(\"k = ---------- \",i)\n",
    "        sel_k = SelectKBest(chi2, k=i)\n",
    "        f_kbest_k = sel_k.fit_transform(f, c)\n",
    "        cm_acc = extract_classify_test(f_kbest_k,c)\n",
    "        cm_accs.append(cm_acc)\n",
    "    \n",
    "    # find max acc\n",
    "    \n",
    "    max_accs = []\n",
    "    for k,cm_acc in enumerate(cm_accs):\n",
    "        max_accs.append(find_max_acc(cm_acc))\n",
    "    max_a = max(max_accs)\n",
    "    print(max_a)\n",
    "    \n",
    "    #find max precision\n",
    "    \n",
    "    #max_prc = []\n",
    "    #for k,cm_acc in enumerate(cm_accs):\n",
    "    #    max_prc.append(find_max_prec(cm_acc))\n",
    "    #max_p = max(max_prc)\n",
    "    #print(max_p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
